---
phase: 05-voice-transcription
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/app/api/transcribe/route.ts
  - app/app/(protected)/transcribe/page.tsx
autonomous: true

must_haves:
  truths:
    - "User can record audio via microphone and get real transcription"
    - "User can upload an audio file and get real transcription"
    - "Transcribed text appears in editable text area"
    - "Transcribed text can be used for report generation"
  artifacts:
    - path: "app/app/api/transcribe/route.ts"
      provides: "Whisper transcription endpoint"
      exports: ["POST"]
      contains: "experimental_transcribe"
    - path: "app/app/(protected)/transcribe/page.tsx"
      provides: "Transcription UI with real API integration"
      contains: "fetch.*api/transcribe"
  key_links:
    - from: "app/app/(protected)/transcribe/page.tsx"
      to: "/api/transcribe"
      via: "FormData POST with audio blob"
      pattern: "fetch.*api/transcribe"
    - from: "app/app/api/transcribe/route.ts"
      to: "openai.transcription"
      via: "Vercel AI SDK experimental_transcribe"
      pattern: "experimental_transcribe"
---

<objective>
Replace mock transcription with real Whisper API integration via Vercel AI SDK.

Purpose: Enable radiologists to transcribe voice dictations using OpenAI Whisper for accurate medical terminology recognition.
Output: Working transcription endpoint and updated UI that calls real Whisper API.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ai-report-generation/04-01-SUMMARY.md

# Existing source files to understand patterns
@app/app/api/generate/route.ts
@app/app/(protected)/transcribe/page.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Whisper transcription API endpoint</name>
  <files>app/app/api/transcribe/route.ts</files>
  <action>
Create POST endpoint at /api/transcribe that accepts FormData with audio file and returns transcription.

Implementation details:
1. Use Node.js runtime (NOT Edge) - file upload with FormData parsing requires Node.js
2. Set maxDuration to 120 seconds (audio files take longer to process)
3. Accept FormData with field name 'audio' containing the audio file
4. Validate file is present and under 25MB
5. Use Vercel AI SDK: `import { experimental_transcribe as transcribe } from 'ai'`
6. Use OpenAI transcription model: `import { openai } from '@ai-sdk/openai'`
7. Call `transcribe({ model: openai.transcription('whisper-1'), audio: buffer })`
8. Return JSON response: `{ success: true, transcript: text, duration: durationInSeconds }`

Authentication:
- Require authenticated user (same pattern as /api/generate)
- Create Supabase server client and verify user

Error handling:
- 401 if not authenticated
- 400 if no audio file or invalid file type
- 413 if file too large (>25MB)
- 500 with helpful message if OPENAI_API_KEY missing
- Generic 500 for other errors

Supported audio formats: mp3, mp4, mpeg, mpga, m4a, wav, webm

Pattern to follow: Use /api/generate/route.ts as reference for auth, error handling, and response format. Key difference: Node.js runtime (not Edge) and FormData parsing instead of JSON body.
  </action>
  <verify>
1. File exists at app/app/api/transcribe/route.ts
2. Contains `export const runtime = 'nodejs'`
3. Contains `experimental_transcribe`
4. Contains `openai.transcription('whisper-1')`
5. Has proper authentication check
6. TypeScript compiles: `cd app && pnpm typecheck` passes for this file
  </verify>
  <done>
POST /api/transcribe endpoint exists with Whisper integration via Vercel AI SDK, proper auth, and error handling.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update transcribe page to call real API</name>
  <files>app/app/(protected)/transcribe/page.tsx</files>
  <action>
Update the transcribe page to call the real /api/transcribe endpoint instead of simulating transcription.

Changes required:

1. **Update stopRecording function:**
   - After MediaRecorder stops and audioBlob is created, call the transcription API
   - Create FormData with the audio blob
   - POST to /api/transcribe with FormData
   - Handle loading state (setIsProcessing)
   - Parse JSON response and set transcribedText
   - Handle errors with toast notifications

2. **Update handleFileUpload function:**
   - Instead of simulating transcription, POST the file to /api/transcribe
   - Same FormData pattern as recording
   - Remove the simulated failure logic (shouldFail, retryCount)
   - Handle real errors from API

3. **Update handleRetryTranscription function:**
   - Actually retry the API call with lastUploadedFile
   - Same pattern as handleFileUpload

4. **Remove simulation code:**
   - Remove the hardcoded sampleTranscription strings
   - Remove Math.random() failure simulation
   - Remove retryCount logic (unless keeping for exponential backoff)

5. **Keep existing UI and features:**
   - Audio playback controls
   - Modality/body part detection
   - Macro expansion
   - YOLO mode integration
   - "Use in Report" flow to /generate page

Error handling:
- Show toast on API errors
- Set transcriptionError state for retry UI
- Preserve existing error display pattern

Pattern reference: See how /generate page.tsx handles streaming API calls with AbortController. Transcription is simpler (not streaming), just POST and await JSON response.
  </action>
  <verify>
1. stopRecording calls fetch('/api/transcribe', { method: 'POST', body: FormData })
2. handleFileUpload calls the same endpoint
3. No hardcoded sampleTranscription strings remain (except for fallback messages)
4. Error handling shows toast and sets transcriptionError
5. Existing UI elements preserved (recording controls, playback, macros, YOLO detection)
6. Build passes: `cd app && pnpm build` succeeds
  </verify>
  <done>
Transcribe page calls real Whisper API. Recording and file upload both use /api/transcribe endpoint. Error handling and existing UI preserved.
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] POST /api/transcribe endpoint exists and returns proper JSON
- [ ] Transcribe page uses real API for both recording and file upload
- [ ] `cd app && pnpm typecheck` passes without errors related to these files
- [ ] `cd app && pnpm build` succeeds
- [ ] Manual test: Start dev server, navigate to /transcribe, upload a test audio file
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No hardcoded mock transcriptions remain
- Whisper API integration works via Vercel AI SDK
- OPENAI_API_KEY environment variable required (same as /generate)
</success_criteria>

<output>
After completion, create `.planning/phases/05-voice-transcription/05-01-SUMMARY.md`
</output>
