---
phase: 12-workspace-consolidation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/components/workspace/report-workspace.tsx
autonomous: false

must_haves:
  truths:
    - "Voice recording sends audio to /api/transcribe and displays result"
    - "Generate Report calls /api/generate with template/findings and streams response"
    - "Template selector fetches from /api/templates/list (real database)"
    - "File upload for audio works (select file triggers transcription)"
  artifacts:
    - path: "app/components/workspace/report-workspace.tsx"
      provides: "Workspace wired to real APIs"
      min_lines: 400
      contains: "fetch.*api/generate"
  key_links:
    - from: "TranscribeTab"
      to: "/api/transcribe"
      via: "FormData POST with audio file"
      pattern: "fetch.*api/transcribe"
    - from: "handleGenerate"
      to: "/api/generate"
      via: "JSON POST with streaming response"
      pattern: "fetch.*api/generate"
    - from: "useEffect for templates"
      to: "/api/templates/list"
      via: "GET request on mount"
      pattern: "fetch.*api/templates/list"
---

<objective>
Wire the workspace buttons to real API endpoints.

Purpose: Transform the workspace from a UI prototype with mock data to a functional tool that actually transcribes audio, generates reports, and loads templates from the database.
Output: ReportWorkspace component connected to /api/transcribe, /api/generate, and /api/templates/list.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
@~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# API routes already implemented:
@app/app/api/transcribe/route.ts
@app/app/api/generate/route.ts
@app/app/api/templates/list/route.ts

# Current workspace component to modify:
@app/components/workspace/report-workspace.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire template selector to real API</name>
  <files>app/components/workspace/report-workspace.tsx</files>
  <action>
    Replace the hardcoded `templates` array with a real API fetch:

    1. Add state for templates and loading:
       ```typescript
       const [templates, setTemplates] = useState<Template[]>([]);
       const [isLoadingTemplates, setIsLoadingTemplates] = useState(true);
       ```

    2. Add useEffect to fetch templates on mount:
       ```typescript
       useEffect(() => {
         async function loadTemplates() {
           try {
             const res = await fetch('/api/templates/list');
             if (res.ok) {
               const data = await res.json();
               setTemplates(data.data.map((t: any) => ({
                 id: t.id,
                 label: t.name,
                 category: t.modality,
                 bodyPart: t.bodyPart,
               })));
             }
           } catch (error) {
             console.error('Failed to load templates:', error);
           } finally {
             setIsLoadingTemplates(false);
           }
         }
         loadTemplates();
       }, []);
       ```

    3. Group templates by modality (category) dynamically instead of hardcoded CT/MRI/X-Ray.

    4. Show loading state in template dropdown while fetching.

    Note: Keep fallback to hardcoded templates if API fails (graceful degradation).
  </action>
  <verify>Template dropdown loads from database when user is authenticated</verify>
  <done>Template selector shows real templates from /api/templates/list</done>
</task>

<task type="auto">
  <name>Task 2: Wire voice transcription to real Whisper API</name>
  <files>app/components/workspace/report-workspace.tsx</files>
  <action>
    Modify TranscribeTab to send audio to the real API:

    1. Add file input ref for audio upload:
       ```typescript
       const fileInputRef = useRef<HTMLInputElement>(null);
       ```

    2. Add audio upload button alongside microphone:
       - Button labeled "Upload Audio"
       - Accepts audio files: .mp3, .wav, .m4a, .webm
       - On file select, call transcribe API

    3. Create transcription handler:
       ```typescript
       async function handleTranscribe(audioBlob: Blob | File) {
         setIsTranscribing(true);
         try {
           const formData = new FormData();
           formData.append('audio', audioBlob);

           const res = await fetch('/api/transcribe', {
             method: 'POST',
             body: formData,
           });

           if (!res.ok) {
             const error = await res.json();
             throw new Error(error.message || 'Transcription failed');
           }

           const data = await res.json();
           onTranscriptionChange(prev => prev ? `${prev}\n\n${data.transcript}` : data.transcript);
         } catch (error) {
           console.error('Transcription error:', error);
           // Show error to user (could use toast)
         } finally {
           setIsTranscribing(false);
         }
       }
       ```

    4. For live recording (microphone button):
       - Use MediaRecorder API to record audio
       - On stop, convert to Blob and call handleTranscribe
       - Show recording state (isRecording) and transcription state (isTranscribing)

    5. Add isTranscribing state with loading indicator during API call.
  </action>
  <verify>Upload an audio file, verify text appears in transcription area</verify>
  <done>Voice recording and file upload both send to /api/transcribe and display result</done>
</task>

<task type="auto">
  <name>Task 3: Wire Generate Report to streaming API</name>
  <files>app/components/workspace/report-workspace.tsx</files>
  <action>
    Replace the mock handleGenerate with real API call:

    1. Update handleGenerate to call the streaming API:
       ```typescript
       const handleGenerate = async () => {
         if (!transcription || !effectiveTemplateId) return;

         setIsGenerating(true);
         setReportContent(''); // Clear previous
         setActiveTab('report');

         const template = templates.find(t => t.id === effectiveTemplateId);

         try {
           const res = await fetch('/api/generate', {
             method: 'POST',
             headers: { 'Content-Type': 'application/json' },
             body: JSON.stringify({
               templateId: effectiveTemplateId,
               templateName: template?.label || 'General',
               modality: template?.category || 'CT',
               bodyPart: template?.bodyPart || 'Chest',
               findings: transcription,
             }),
           });

           if (!res.ok) {
             const error = await res.json();
             throw new Error(error.message || 'Generation failed');
           }

           // Stream the response
           const reader = res.body?.getReader();
           const decoder = new TextDecoder();

           while (reader) {
             const { done, value } = await reader.read();
             if (done) break;

             const chunk = decoder.decode(value);
             setReportContent(prev => (prev || '') + chunk);
           }
         } catch (error) {
           console.error('Generation error:', error);
           setReportContent('Error generating report. Please try again.');
         } finally {
           setIsGenerating(false);
         }
       };
       ```

    2. Show streaming text in Report tab as it arrives.

    3. Disable Generate button when:
       - No transcription text
       - No template selected
       - Currently generating
  </action>
  <verify>Click Generate, verify streaming text appears in report area</verify>
  <done>Generate Report streams real GPT-4o output to Report tab</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Workspace fully wired to real APIs: templates load from database, transcription uses Whisper, report generation streams from GPT-4o</what-built>
  <how-to-verify>
    1. Run: `cd app && npm run dev`
    2. Visit: http://localhost:3000/dashboard
    3. Verify template dropdown shows templates from database (not hardcoded CT/MRI/X-Ray)
    4. Test file upload: Click "Upload Audio", select an audio file, verify transcription appears
    5. Test generation: With transcription text present, select a template, click "Generate Report"
    6. Verify streaming: Report text should appear progressively (streaming), not all at once
    7. Check both tabs work: Voice Input shows transcription, Report shows generated content
  </how-to-verify>
  <resume-signal>Type "approved" to continue, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd app && npx tsc --noEmit` passes
- [ ] `cd app && npm run build` succeeds
- [ ] Templates load from /api/templates/list
- [ ] Audio upload transcribes via /api/transcribe
- [ ] Generate button streams from /api/generate
- [ ] Human verification passed
</verification>

<success_criteria>
- All tasks completed
- TypeScript compiles without errors
- Template selector shows real database templates
- Audio upload triggers real transcription
- Generate Report streams real AI output
- Human verified functionality works end-to-end
</success_criteria>

<output>
After completion, create `.planning/phases/12-workspace-consolidation/12-02-SUMMARY.md`
</output>
