---
phase: 04-ai-report-generation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/package.json
  - app/app/api/generate/route.ts
  - app/app/(protected)/generate/page.tsx
autonomous: true

must_haves:
  truths:
    - "User can enter clinical findings and click generate"
    - "User sees streaming text appear progressively as AI generates"
    - "Generated report displays in section-based format (FINDINGS, IMPRESSION, etc.)"
    - "Report generation uses selected template for structure guidance"
    - "Credits are deducted after successful generation"
  artifacts:
    - path: "app/app/api/generate/route.ts"
      provides: "SSE streaming endpoint for GPT-4o report generation"
      exports: ["POST"]
      min_lines: 80
    - path: "app/app/(protected)/generate/page.tsx"
      provides: "Updated frontend consuming real AI stream"
      contains: "useChat\\|fetchEventSource\\|EventSource"
  key_links:
    - from: "app/app/(protected)/generate/page.tsx"
      to: "/api/generate"
      via: "POST with streaming response"
      pattern: "fetch.*api/generate\\|useChat.*api.*generate"
    - from: "app/app/api/generate/route.ts"
      to: "OpenAI GPT-4o"
      via: "Vercel AI SDK streamText"
      pattern: "streamText.*openai\\|openai.*gpt-4o"
---

<objective>
Implement real GPT-4o streaming report generation to replace the simulated AI responses.

Purpose: Enable radiologists to generate real AI-powered reports from clinical findings with real-time streaming feedback.
Output: Working API endpoint with SSE streaming, updated frontend consuming the stream, real GPT-4o integration.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/INTEGRATIONS.md

# Prior phase summaries (template API patterns)
@.planning/phases/03-template-system/03-01-SUMMARY.md

# Source files to modify
@app/app/(protected)/generate/page.tsx
@app/app/api/templates/route.ts (reference for API patterns)
@app/package.json
@app/.env.example
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Vercel AI SDK and OpenAI provider</name>
  <files>app/package.json</files>
  <action>
    Install the Vercel AI SDK and OpenAI provider packages:

    ```bash
    cd app && pnpm add ai @ai-sdk/openai
    ```

    The AI SDK provides:
    - `streamText()` for streaming text generation
    - `@ai-sdk/openai` for the OpenAI provider with GPT-4o support

    Do NOT install `openai` directly - use the AI SDK wrapper which handles streaming properly.
  </action>
  <verify>
    - Check package.json contains "ai" and "@ai-sdk/openai" in dependencies
    - Run `pnpm install` completes without errors
  </verify>
  <done>AI SDK packages installed and available for import</done>
</task>

<task type="auto">
  <name>Task 2: Create streaming report generation API endpoint</name>
  <files>app/app/api/generate/route.ts</files>
  <action>
    Create POST /api/generate endpoint with Edge runtime for low latency streaming.

    **Request body:**
    ```typescript
    {
      templateId: string;
      findings: string;
      templateName: string;
      modality: string;
      bodyPart: string;
      templateContent?: string; // Optional template structure hints
    }
    ```

    **Implementation requirements:**
    1. Set `export const runtime = 'edge'` for Edge runtime
    2. Set `export const maxDuration = 30` for 30s timeout
    3. Authenticate user via Supabase server client
    4. Validate request body with Zod
    5. Use `streamText()` from AI SDK with `openai('gpt-4o')` model
    6. Use low temperature (0.2) for deterministic medical reports
    7. Create a system prompt that structures the report with sections:
       - CLINICAL INDICATION (echo user's findings)
       - TECHNIQUE (based on modality)
       - FINDINGS (AI-generated based on clinical findings)
       - IMPRESSION (AI-generated summary)
    8. Return `result.toDataStreamResponse()` for streaming
    9. Handle errors with proper status codes (401, 400, 500)

    **System prompt structure:**
    ```
    You are an expert radiologist assistant generating structured radiology reports.

    Template: {templateName}
    Modality: {modality}
    Body Part: {bodyPart}

    Generate a professional radiology report with these sections:
    - CLINICAL INDICATION: Summarize the provided clinical findings
    - TECHNIQUE: Describe standard technique for {modality} of {bodyPart}
    - FINDINGS: Provide detailed findings based on the clinical indication
    - IMPRESSION: Provide a concise summary and recommendations

    Be thorough, professional, and use standard radiological terminology.
    ```

    **Error handling:**
    - 401 if not authenticated
    - 400 if validation fails
    - 500 for OpenAI errors (with generic message to client, detailed log server-side)

    **Important:** Use `process.env.OPENAI_API_KEY` (already configured in .env.example)
  </action>
  <verify>
    - File exists at app/app/api/generate/route.ts
    - TypeScript compiles: `cd app && pnpm typecheck` (may have pre-existing errors)
    - Manual test with curl (if OPENAI_API_KEY is set):
      ```
      curl -X POST http://localhost:3000/api/generate \
        -H "Content-Type: application/json" \
        -d '{"templateId":"test","findings":"chest pain","templateName":"Test","modality":"CT","bodyPart":"Chest"}'
      ```
  </verify>
  <done>API endpoint created with Edge runtime, Zod validation, and GPT-4o streaming</done>
</task>

<task type="auto">
  <name>Task 3: Update generate page to consume streaming response</name>
  <files>app/app/(protected)/generate/page.tsx</files>
  <action>
    Update the generate page to call the real API and render streaming text progressively.

    **Changes required:**

    1. **Replace simulation with real fetch:**
       - Remove the simulated chunk loop (lines ~413-428)
       - Replace with fetch to /api/generate with streaming response

    2. **Streaming consumption pattern:**
       ```typescript
       const response = await fetch('/api/generate', {
         method: 'POST',
         headers: { 'Content-Type': 'application/json' },
         body: JSON.stringify({
           templateId: selectedTemplateId,
           findings,
           templateName: template?.name || 'Unknown',
           modality: template?.modality || 'Unknown',
           bodyPart: template?.bodyPart || 'Unknown',
           templateContent: template?.content
         }),
         signal: abortController.signal // For cancellation support
       });

       if (!response.ok) {
         const error = await response.json();
         throw new Error(error.message || 'Generation failed');
       }

       const reader = response.body?.getReader();
       const decoder = new TextDecoder();
       let accumulated = '';

       while (true) {
         const { done, value } = await reader.read();
         if (done) break;

         // Decode SSE data
         const chunk = decoder.decode(value, { stream: true });
         // Parse AI SDK data stream format
         const lines = chunk.split('\n');
         for (const line of lines) {
           if (line.startsWith('0:')) {
             // Text delta from AI SDK
             const text = JSON.parse(line.slice(2));
             accumulated += text;
             setGeneratedReport(accumulated);
           }
         }
       }
       ```

    3. **Preserve existing functionality:**
       - Keep AbortController for cancellation (handleCancelGeneration)
       - Keep credit deduction after successful generation
       - Keep error toast notifications
       - Keep draft clearing on success

    4. **Error handling:**
       - Show toast for network errors
       - Show toast for API errors (use message from response)
       - Handle AbortError gracefully (user cancelled)

    5. **Loading state:**
       - Set isGenerating=true before fetch
       - Set isGenerating=false after stream completes or errors
       - Update generatedReport progressively as stream arrives

    **Do NOT change:**
    - Template loading (uses Supabase browser client)
    - Draft persistence (localStorage + IndexedDB)
    - Export functionality (PDF/Word)
    - Section parsing and regeneration
    - Credits display and no-credits dialog
  </action>
  <verify>
    - TypeScript compiles without new errors
    - Dev server starts: `cd app && pnpm dev`
    - Navigate to /generate page (should load without errors)
    - If OPENAI_API_KEY is configured: Test full generation flow
  </verify>
  <done>Generate page consumes real streaming API, shows progressive text, supports cancellation</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `cd app && pnpm install` succeeds
- [ ] `cd app && pnpm typecheck` runs (pre-existing errors acceptable, no NEW errors)
- [ ] `cd app && pnpm dev` starts dev server
- [ ] /api/generate route exists and exports POST handler
- [ ] Generate page loads without JavaScript errors
- [ ] Generate page shows "Generating..." state when generate button clicked
- [ ] (If OPENAI_API_KEY set) Full end-to-end generation works with streaming
</verification>

<success_criteria>

- All tasks completed
- AI SDK packages installed
- API endpoint created with Edge runtime and GPT-4o streaming
- Frontend updated to consume streaming response
- Cancellation still works
- Error handling shows appropriate toasts
- Credits still deducted after successful generation
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-report-generation/04-01-SUMMARY.md`
</output>
